{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title: Sentiment Analysis in Nigerian Pidgin English Using DistilBERT\n",
    "#### Abdulkadir Bala Richard (Student ID: 3747307)\n",
    "#### Chijioke Onyeka Ahanwa (Student ID: 3741164)\n",
    "#### David Osawese Okundigie (Student ID: 3754299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description: This script is designed for performing natural language processing tasks using the DistilBert model.\n",
    "#### The script uses the Transformers, Datasets, and Accelerate libraries to facilitate model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script is a conversion of a Jupyter notebook for NLP tasks using transformers.\n",
    "Original notebook located at: https://colab.research.google.com/drive/1UOwPu72DG70WJbg_tRtxoTI_h3vrQBSQ\n",
    "\"\"\"\n",
    "\n",
    "# Installing necessary libraries\n",
    "!pip install datasets evaluate accelerate -U\n",
    "!pip install transformers[torch]\n",
    "\n",
    "# Importing essential libraries for NLP tasks\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import accelerate\n",
    "import evaluate\n",
    "\n",
    "# Checking the version of the transformers library\n",
    "!pip show transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Label Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Loading training, development, and test datasets from TSV files\n",
    "train_df = pd.read_csv('pcm_train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('pcm_dev.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('pcm_test.tsv', sep='\\t')\n",
    "\n",
    "# Mapping textual labels to numerical format for consistency\n",
    "# 'positive': 0, 'neutral': 1, 'negative': 2\n",
    "label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "train_df['label'] = train_df['label'].map(label_mapping)\n",
    "dev_df['label'] = dev_df['label'].map(label_mapping)\n",
    "test_df['label'] = test_df['label'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Conversion and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pandas dataframes to Hugging Face 'datasets' format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Initializing the tokenizer from the DistilBert model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Defining a function for tokenization\n",
    "def tokenize_function(examples):\n",
    "    # Tokenizing the text data with appropriate padding and truncation\n",
    "    return tokenizer(examples['tweet'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "# Applying the tokenization function to the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the evaluate library for metrics\n",
    "import evaluate\n",
    "\n",
    "# Loading metrics for evaluation\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Defining a function to compute metrics during model evaluation\n",
    "def compute_metrics(p):\n",
    "    # Calculating various F1 scores and accuracy\n",
    "    return {\n",
    "        'micro_f1': f1_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='micro'),\n",
    "        'macro_f1': f1_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='macro'),\n",
    "        'weighted_f1': f1_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='weighted'),\n",
    "        'accuracy': accuracy_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Initialization and Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary classes from the transformers library\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Initializing the model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Setting up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Configuring batch sizes for training and evaluation\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    # Other training arguments...\n",
    ")\n",
    "\n",
    "# Note: The '...' above indicates where additional training arguments would be specified,\n",
    "# such as learning rate, number of epochs, etc. These are crucial for controlling the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration Completion and Trainer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completing the training arguments configuration\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initializing the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Other possible configurations...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with compute metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Initiating the training process\n",
    "trainer.train()\n",
    "\n",
    "# Evaluating the model on the development set\n",
    "results = trainer.evaluate(dev_dataset)\n",
    "\n",
    "# Printing evaluation results\n",
    "print(\"Micro F1 on Test Set:\", results[\"eval_micro_f1\"])\n",
    "print(\"Macro F1 on Test Set:\", results[\"eval_macro_f1\"])\n",
    "print(\"Weighted F1 on Test Set:\", results[\"eval_weighted_f1\"])\n",
    "\n",
    "# Note: Uncomment the following line to evaluate on the test dataset after finalizing the model.\n",
    "# results = trainer.evaluate(test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
