{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title: Sentiment Analysis in Nigerian Pidgin English Using DistilBERT\n",
    "#### Abdulkadir Bala Richard (Student ID: 3747307)\n",
    "#### Chijioke Onyeka Ahanwa (Student ID: 3741164)\n",
    "#### David Osawese Okundigie (Student ID: 3754299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Description: This script is designed for performing natural language processing tasks using the DistilBert model.\n",
    "#### The script uses the Transformers, Datasets, and Accelerate libraries to facilitate model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7b2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: evaluate in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: accelerate in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: packaging in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: psutil in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in /Users/syphonphilter/.local/lib/python3.11/site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.18.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (2.1.1)\n",
      "\u001b[33mWARNING: transformers 2.1.1 does not provide the extra 'torch'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: boto3 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (1.24.28)\n",
      "Requirement already satisfied: requests in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: regex in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: sentencepiece in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.1.99)\n",
      "Requirement already satisfied: sacremoses in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from transformers[torch]) (0.0.43)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from boto3->transformers[torch]) (1.27.59)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from boto3->transformers[torch]) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from boto3->transformers[torch]) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from sacremoses->transformers[torch]) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from sacremoses->transformers[torch]) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from sacremoses->transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/syphonphilter/anaconda3/lib/python3.11/site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers[torch]) (2.8.2)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/syphonphilter/Projects/Sentiment-Analysis-of-Pidgin-English-using-DistilBERT/sentiment_alanysis_nlp_pidgin.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/syphonphilter/Projects/Sentiment-Analysis-of-Pidgin-English-using-DistilBERT/sentiment_alanysis_nlp_pidgin.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install transformers[torch]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/syphonphilter/Projects/Sentiment-Analysis-of-Pidgin-English-using-DistilBERT/sentiment_alanysis_nlp_pidgin.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Importing essential libraries for NLP tasks\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/syphonphilter/Projects/Sentiment-Analysis-of-Pidgin-English-using-DistilBERT/sentiment_alanysis_nlp_pidgin.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/syphonphilter/Projects/Sentiment-Analysis-of-Pidgin-English-using-DistilBERT/sentiment_alanysis_nlp_pidgin.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/syphonphilter/Projects/Sentiment-Analysis-of-Pidgin-English-using-DistilBERT/sentiment_alanysis_nlp_pidgin.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "# Installing necessary libraries\n",
    "!pip install datasets evaluate accelerate -U\n",
    "!pip install transformers[torch]\n",
    "\n",
    "# Importing essential libraries for NLP tasks\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import accelerate\n",
    "import evaluate\n",
    "\n",
    "# Checking the version of the transformers library\n",
    "!pip show transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Label Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Loading training, development, and test datasets from TSV files\n",
    "# Importing pandas for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Loading training, development, and test datasets from TSV files\n",
    "train_df = pd.read_csv('https://github.com/Syphonphilter/afrisent-semeval-2023/tree/main/data/pcm/train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('https://github.com/afrisenti-semeval/afrisent-semeval-2023/blob/main/data/pcm/dev.tsv', sep='\\t')\n",
    "test_df = pd.read_csv('https://github.com/Syphonphilter/afrisent-semeval-2023/tree/main/data/pcm/test.tsv', sep='\\t')\n",
    "\n",
    "# Mapping textual labels to numerical format for consistency\n",
    "# 'positive': 0, 'neutral': 1, 'negative': 2\n",
    "label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "train_df['label'] = train_df['label'].map(label_mapping)\n",
    "dev_df['label'] = dev_df['label'].map(label_mapping)\n",
    "test_df['label'] = test_df['label'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Conversion and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting pandas dataframes to Hugging Face 'datasets' format\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "dev_dataset = Dataset.from_pandas(dev_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Initializing the tokenizer from the DistilBert model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Defining a function for tokenization\n",
    "def tokenize_function(examples):\n",
    "    # Tokenizing the text data with appropriate padding and truncation\n",
    "    return tokenizer(examples['tweet'], padding='max_length', truncation=True, max_length=256)\n",
    "\n",
    "# Applying the tokenization function to the datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "dev_dataset = dev_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metrics Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the evaluate library for metrics\n",
    "import evaluate\n",
    "\n",
    "# Loading metrics for evaluation\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Defining a function to compute metrics during model evaluation\n",
    "def compute_metrics(p):\n",
    "    # Calculating various F1 scores and accuracy\n",
    "    return {\n",
    "        'micro_f1': f1_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='micro'),\n",
    "        'macro_f1': f1_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='macro'),\n",
    "        'weighted_f1': f1_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids, average='weighted'),\n",
    "        'accuracy': accuracy_metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Initialization and Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary classes from the transformers library\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Initializing the model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "# Setting up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    # Configuring batch sizes for training and evaluation\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    # Other training arguments...\n",
    ")\n",
    "\n",
    "# Note: The '...' above indicates where additional training arguments would be specified,\n",
    "# such as learning rate, number of epochs, etc. These are crucial for controlling the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration Completion and Trainer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completing the training arguments configuration\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Initializing the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Other possible configurations...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with compute metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Initiating the training process\n",
    "trainer.train()\n",
    "\n",
    "# Evaluating the model on the development set\n",
    "results = trainer.evaluate(dev_dataset)\n",
    "\n",
    "predicted_labels = results.predictions.argmax(-1)\n",
    "true_labels = results.label_ids\n",
    "\n",
    "\n",
    "# Printing evaluation results\n",
    "print(\"Micro F1 on Test Set:\", results[\"eval_micro_f1\"])\n",
    "print(\"Macro F1 on Test Set:\", results[\"eval_macro_f1\"])\n",
    "print(\"Weighted F1 on Test Set:\", results[\"eval_weighted_f1\"])\n",
    "\n",
    "# Note: Uncomment the following line to evaluate on the test dataset after finalizing the model.\n",
    "# results = trainer.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Confision Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have true labels and predictions from your model\n",
    "\n",
    "# Generating the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
